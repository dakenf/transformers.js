
State-of-the-art Machine Learning for the web. Run ü§ó Transformers directly in your browser, with no need for a server!

Transformers.js is designed to be functionally equivalent to Hugging Face's [transformers](https://github.com/huggingface/transformers) python library, meaning you can run the same pretrained models using a very similar API. These models support common tasks in different modalities, such as:
  - üìù **Natural Language Processing**: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.
  - üñºÔ∏è **Computer Vision**: image classification, object detection, and segmentation.
  - üó£Ô∏è **Audio**: automatic speech recognition and audio classification.
  - üêô **Multimodal**: zero-shot image classification and text-to-image generation.

Transformers.js uses [ONNX Runtime](https://onnxruntime.ai/) to run models in the browser. The best part about it, is that you can easily [convert](/custom_usage#convert-your-models-to-onnx) your pretrained PyTorch, TensorFlow, or JAX models to ONNX using [ü§ó Optimum](https://github.com/huggingface/optimum#onnx--onnx-runtime). 

For more information, check out the full [documentation](https://huggingface.co/docs/transformers.js).

## GPU acceleration in Node.js:

**Windows and macOS**:

Works out of the box

**Linux**:
1. Install CUDA https://docs.nvidia.com/cuda/cuda-installation-guide-linux/
2. Install cuDNN https://developer.nvidia.com/rdp/cudnn-archive
3. Install onnxruntime-linux-x64-gpu-1.14.1 https://github.com/microsoft/onnxruntime/releases/tag/v1.14.1
